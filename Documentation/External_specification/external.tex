% \chapter{Część techniczna/praktyczna}

% \chapter{Wymagania i narzędzia}

\chapter{[Właściwy dla kierunku -- np. Specyfikacja zewnętrzna]}
% Jeśli to Specyfikacja zewnętrzna:
% \begin{itemize}
% \item  wymagania sprzętowe i programowe
% \item  sposób instalacji
% \item  sposób aktywacji
% \item  kategorie użytkowników
% \item  sposób obsługi
% \item   administracja systemem
% \item  kwestie bezpieczeństwa
% \item  przykład działania
% \item  scenariusze korzystania z systemu (ilustrowane zrzutami z ekranu lub generowanymi dokumentami)
% \end{itemize}

% \section{Funkcjonalność modułu}
% \quad Paczkę można wykorzystać tam, gdzie wymagane jest wykorzystanie gestów oraz ruchów dłoni. Paczka może znnaaleźć zastosowanie aplikacjach użytku codziennego, robotyce lub pełnić formę peryferium komputerowego.  

\section{Wymagania sprzętowe}
\subsection{Kamera}
\quad Elementem niezbędnym do korzystania z modułu OpenLeap jest kamera, która pozwoli na pozyskanie obrazu. Rozdzielczość matrycy kamery powinna być wystarczająco duża, aby pozwolić na rozpoznanie dłoni. Nie ma tutaj minimalnych wymagań, większa rozdzielczość, czy też lepsza praca w warunkach niskiego oświetlenia kamery pozwoli na poprawniejsze działania algorytmów identyfikujących dłoń. Podobnie ma się liczba klatek na sekundę, która powinna być wystarczająco wysoka, tak aby można było sprawnie korzystać z możliwości oprogramowania. 

\subsection{Komputer}
\quad Jednostka obliczeniowa powinna zostać wyposażona w system operacyjny dający możliwość obsługi języka Python, taki warunek spełnia większość systemów operacyjnych na rynku. Komputer powinien spełniać minimalne wymagania w kwestii wydajności przetwarzania obrazu. Fakt istnienia możliwości instalacji i wykorzystania biblioteki MediaPipe przez minikomputery Raspberry Pi w wersji 3 i 4 oznacza, że wymagania nie są wysokie. 


\section{Instalacja paczki}
\quad Instalacja paczki odbywa się poprzez wykorzystanie programu \textbf{pip}, który jest instalowany automatycznie razem z językiem Python. W zależności od wybranego systemu operacyjnego komenda może przybierać różne formy, ogólnie można przyjąć poniższy zapis \ref{lst:installcom}. Komenda powinna zostać wykonane poprzez powłokę \textbf{bash} lub inną dostępną w systemie Unix-owym lub poprzez wiersz poleceń w systemie Windows. 

\begin{lstlisting}[language=bash, style=command, label={lst:installcom}, caption={Instalacja paczki}]
    $ pip install openleap
\end{lstlisting}

\quad Informacje na temat biblioteki można znaleźć na platformie PyPi pod linkiem: \textbf{\href{https://pypi.org/project/openleap/}{https://pypi.org/project/openleap/}}. Na tej stronie znajduje się opis modułu, instrukcja instalacji oraz przykładowe programy i możliwości wykorzystania.

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=15cm]{../images/pypi_page.png}
        \caption{Strona modułu OpenLeap na PyPi}
    \end{center}
\end{figure}

\section{Program testowy}

\quad Paczkę można przetestować korzystając z dostępnych metod klasy. W ramach testu istnieje możliwość napisania programu wyłącznie w powłoce języka Python. W pierwszym kroku programu zostaje zaimportowana paczka \textbf{openleap}. Zostaje stworzony obiekt kontrolera z wybranymi parametrami, dokładny opis parametrów znajduje się w późniejszej części rozdziału, w podsekcji (\ref{parametry}). Metoda \textbf{loop()} odpowiada za wywołanie głównej logiki programu odpowiedzialnej za generowanie danych oraz wyświetlanie ich w wybranym miejscu (powłoka, okno graficzne).\newline

\begin{lstlisting}[language=python, style=programming, label={lst:simple_program}, caption={Program testowy}]
    import openleap

    controller = openleap.OpenLeap(screen_show=True, 
                                   screeen_type='BLACK', 
                                   show_data_on_image=True, 
                                   gesture_model='basic')
    
    controller.loop()
\end{lstlisting}

\quad Przykładowy program pozwoli na wyświetlenie okna z widocznymi dłońmi wraz z oznaczonymi punktami charakterystycznymi oraz opisem parametrów, takich jak obrót dłoni względem nadgarstka, jej pozycja względem lewego górnego rogu obrazu kamery czy rozpoznany gest. Zrzut ekranu testowego programu znajduje się poniżej na rysunku \ref{fig:prog_screen_1}. 

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=9cm]{../images/example_program.png}
        \caption{Zrzut ekranu programu testowego}
        \label{fig:prog_screen_1}
    \end{center}
\end{figure}

\quad Dane zostają zapisane w słowniku składającym się z obiektów typu \textbf{dataclass}, które przechowują wygenerowane informacje o danej dłoni. Operację pobrania danych można wykonać w poniżej przedstawiony sposób. Klasa \textbf{dataclass} zostanie dokładniej opisana w kolejnym rozdziale.

\begin{lstlisting}[language=python, style=programming, caption={Odczyt gestów}, label={lst:get_data1}]
    if controller.data['right'].gesture == 'open':
        print('Right hand is opened!')
    elif controller.data['right'].gesture == 'fist':
        print('Right hand is closed!')
\end{lstlisting}

\quad Program oferuje możliwość odczytu odległości między końcem palca wskazującego a końcem kciuka. Te punkty zostały oznaczone poprzez niebieską linię oraz wartość \textbf{distance}, co można zobaczyć na rysunku \ref{fig:prog_screen_1}. Przykład odczytu tej wartości znajduje się w poniższym przykładzie. 

\begin{lstlisting}[language=python, style=programming, caption={Odczyt edległości między palcami}]
    if controller.data['right'].distance < 20:
        print('Click has been detected!')
\end{lstlisting}

\subsection{Parametry}
\label{parametry}

\quad Obiekt klasy \textbf{openleap} przyjmuje za argumenty inicjalizatora parametry, określające działanie programu. Parametry określają czy należy wyświetlić okno graficzne, wartości obliczanych danych oraz model rozpoznający gesty. Wszystkie atrybuty klasy zostaną dokładnie opisane w kolejnym rozdziale. W tej sekcji zostały opisane jedynie parametry inicjalizatora wraz ze swoimi typami.\newline

Parametry Inicjalizatora:
\begin{enumerate}
    \item \textbf{screen\_show} -- podgląd okna (boolean)
    \item \textbf{screen\_type} -- typ tła wyświetlany w oknie graficznym (string)
    \begin{itemize}
        \item \enquote{cam} -- obraz z kamery
        \item \enquote{black} -- czarne tło
    \end{itemize}
    \item \textbf{show\_data\_in\_console} -- wyświetlanie danych w konsoli (boolean)
    \item \textbf{show\_data\_on\_image} -- wyświetlanie danych w oknie graficznym (boolean)
    \item \textbf{normalized\_position} -- wyświetlanie znormalizowanej pozycji (boolean)
    \item \textbf{gesture\_model} -- wybór modelu rozpoznającego gesty (string)
    \begin{itemize}
        \item \enquote{basic} -- gesty podstawowe
        \item \enquote{sign\_language} -- język migowy
    \end{itemize}
    \item \textbf{lr\_mode} -- metoda rozpoznawania typu dłoni (string)
    \begin{itemize}
        \item \enquote{AI} -- wykorzystanie algorytmów klasyfikacji 
        \item \enquote{position} -- określenie typu na podstawie pozycji dłoni według siebie
    \end{itemize}
    \item \textbf{activate\_data} -- warunek odpowiadający za ciągły zapis do struktury danych \textbf{data}, opisanej w przykładzie programu \ref{lst:get_data1}
\end{enumerate}


\begin{figure}[H]
    \centering
    \subfloat[Czarne tło.]{\includegraphics[width=7.3cm]{../images/example_program.png}\label{par_bg:f1}}
    \hfill
    \subfloat[Obraz z kamery.]{\includegraphics[width=7.3cm]{../images/cam_example.png}\label{par_bg:f2}}
    \caption{Parametr określający typ tła.}
\end{figure}

% \subsection{Dostępne funkcje}

% \subsection{Główny wątek}
% \quad Działanie kontrolera opiera się na wywoływaniu funkcji \textbf{main()}. Tę funkcję można wywołać za pomocą wybudowanej funkcji \textbf{loop()}, która po prostu wywołuje funkcję \textbf{main()} w pętli wraz z warunkiem wyjścia z programu. Takie podejście może ułatwić stosowanie kontrolera w osobnym wątku aplikacji. 
% \quad Alternatywnym podejściem jest po prostu zastosowanie funkcji \textbf{main()} w pętli budowanego programu. Takie rozwiązanie daje podobny rezultat, ale bez konieczności wykorzystania osobnego wątku. 

\section{Przykłady użycia}
\subsection{Rozpoznawanie alfabetu w języku migowym}
\quad Pierwszym przykładem zastosowania jest wykorzystanie paczki do rozpoznawania alfabetu języka migowego. Taki program może umożliwić komunikację między osobą głuchoniemą posługującą się językiem migowym a osobą, która takiego języka nie zna. Przygotowany model rozpoznaje litery przedstawione na poniższej grafice \ref{img:alphabet}. Dzięki wykorzystanym algorytmom uczenia maszynowego model potrafi rozpoznać skomplikowane ułożenie dłoni symbolizujące daną literę.  


\begin{figure}[H]
    \begin{center}
        \includegraphics[width=15cm]{../images/american_sign_language.jpg}
        \caption{Gesty alfabetu języka migowego}
        \label{img:alphabet}
    \end{center}
\end{figure}

\quad Oprócz modelu rozpoznającego gesty alfabetu języka migowego klasa została wyposażona w model rozpoznający podstawowe gesty, czyli gesty otwartej i zamkniętej dłoni. Wybór odpowiedniego modelu odbywa się w momencie tworzenia obiektu poprzez ustawienie parametru \textbf{gesture\_model}. Programista może wybrać model, który lepiej sprawdzi się w tworzonej aplikacji. 


\begin{figure}[H]
    \centering
    \subfloat[Litery A i B.]{\includegraphics[width=7.3cm]{../images/a_b.png}\label{gest_pag:f1}}
    \hfill
    \subfloat[Litery C i X.]{\includegraphics[width=7.3cm]{../images/c_x.png}\label{gest_pag:f2}}
    \hfill
    \subfloat[Litery F i O.]{\includegraphics[width=7.3cm]{../images/f_o.png}\label{gest_pag:f3}}
    \caption{Rozpoznawanie języka migowego.}
\end{figure}

\subsection{Interaktywny Kiosk}
\quad Kolejnym przykładem jest Interaktywny kiosk, który pozwala na złożenie zamówienia w sposób, który nie wymaga dotykania ekranu dotykowego. Warunkiem komfortowego korzystania z aplikacji jest wysoka liczba klatek na sekundę generowana przez kamerę, co wpływa na płynność ruchu kursora. 

\quad W dobie pandemii, ale i nie tylko takie rozwiązanie może potencjalnie przyczynić się do spowolnienia rozprzestrzeniania się różnego rodzaju wirusów i drobnoustrojów, ponieważ użytkownicy nie muszą dotykać ekranu. 

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=15cm]{../images/checkout_window.png}
        \caption{Zrzut ekranu kiosku interaktywnego}
    \end{center}
\end{figure}

\quad Wskaźnik kiosku interaktywnego jest sterowany poprzez pozycję dłoni, która jest definiowana poprzez pozycję nadgarstka. 

\quad Kliknięcie przycisku zostaje aktywowane poprzez wykrycie odpowiednio małej odległości między końcówką palca wskazującego a końcówką kciuka. 

\quad Odległość między tymi dwoma punktami jest obliczana automatycznie poprzez wbudowaną funkcję klasy OpenLeap. 

\subsection{Dobór koloru}
\quad Ostatnim przykładem jest wykorzystanie dłoni jako kontrolera, za którego pomocą można wybrać dowolny kolor. Takie zastosowanie może zostać wykorzystane w pracy grafika komputerowego. Dzięki temu użytkownik będzie mógł zmieniać kolor wykorzysytywanego narzędzia bez przerywania pracy, na przykład malowania. Kolor można ustawiać tyko wtedy kiedy gest lewej ręki jest gestem otwartej dłoni. 


\begin{figure}[H]
    \centering
    \subfloat[Widok dłoni.]{\includegraphics[width=7.3cm]{../images/yellow_hand_deg.png}\label{30_deg:f1}}
    \hfill
    \subfloat[Dobrany kolor.]{\includegraphics[width=7.3cm]{../images/yellow.png}\label{30_deg:f2}}
    \caption{Obrót dłoni o 30 stopni.}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[Widok dłoni.]{\includegraphics[width=7.3cm]{../images/blueish_hand_deg.png}\label{96_deg:f1}}
    \hfill
    \subfloat[Dobrany kolor.]{\includegraphics[width=7.3cm]{../images/blueish.png}\label{96_deg:f2}}
    \caption{Obrót dłoni o 96 stopni.}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[Widok dłoni.]{\includegraphics[width=7.3cm]{../images/pink_hand_deg.png}\label{174_deg:f1}}
    \hfill
    \subfloat[Dobrany kolor.]{\includegraphics[width=7.3cm]{../images/pink.png}\label{174_deg:f2}}
    \caption{Obrót dłoni o 174 stopni.}
\end{figure}

\quad Dodatkową opcją jest ustawienie saturacji poprzez obliczenie jej wartości na podstawie odległości między palcem wskazującym, a kciukiem. 

\begin{figure}[H]
    \centering
    \subfloat[Widok dłoni.]{\includegraphics[width=7.3cm]{../images/zero_sat_hand_deg.png}\label{sat:f1}}
    \hfill
    \subfloat[Dobrany kolor.]{\includegraphics[width=7.3cm]{../images/zero_sat.png}\label{sat:f2}}
    \caption{Ustawienie saturacji}
\end{figure}


\quad Podobnie jak obliczenie odległości między palcami, biblioteka oblicza obrót dłoni wokół osi Z punktu opisującego pozycję nadgarstka.


\section{Stworzenie nowego modelu}

\quad Użtkownik biblioteki może stworzyć własny model rozpoznający gesty. Wykorzystać do tego można program napisany przy pomocy Jupyter Notebook. Plik jest dostępny na platformie \href{https://bit.ly/3J72U0z}{\textbf{GitHub}}. Plik posiada rozszerzenie \textbf{.ipynb}. Link do modelu: \href{https://bit.ly/3J72U0z}{https://bit.ly/3J72U0z}

\quad Plik został przygotowany w formie instrukcji, tak aby osoba korzystająca mogła krok po kroku zebrać oraz przygotować dane, a później wytrenować model i go zapisać. Zapisany model może zostać ponownie wykorzystany w programie podając go za argument \textbf{gesture\_model}